<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="A tool for efficient semi-supervised video object segmentation (great results with minimal manual labor) and a dataset for benchmarking.">
  <meta property="og:title" content="XMem++: Production-level Video Segmentation From Few Annotated Frames"/>
  <meta property="og:description" content="A tool for efficient semi-supervised video object segmentation (great results with minimal manual labor) and a dataset for benchmarking."/>
  <meta property="og:url" content="https://max810.github.io/xmem2-project-page/"/>
  <!-- Path to banner image, should be in the path listed below. OpFtimal dimenssions are 1200X630-->
  <meta property="og:image" content="https://max810.github.io/xmem2-project-page/static/images/architecture_explanations.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="528"/>


  <meta name="twitter:title" content="XMem++: Production-level Video Segmentation From Few Annotated Frames">
  <meta name="twitter:description" content="A tool for efficient semi-supervised video object segmentation (great results with minimal manual labor) and a dataset for benchmarking.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://max810.github.io/xmem2-project-page/static/images/architecture_explanations.jpg">
  <meta name="twitter:card" content="XMem++ has a simple and powerful GUI to help to segment and annotate complex objects in videos.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="segmentation, interactive segmentation, annotation, annotation tool, data annotation, video segmentation, video object segmentation, XMem, XMem++, IVOS, SSVOS, DAVIS, VOS, dataset, benchmark, semi-supervised video object segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>XMem++: Production-level Video Segmentation From Few Annotated Frames</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">XMem++: Production-level Video Segmentation From Few Annotated Frames</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/maksym-bekuzarov-947490165/" target="_blank">Maksym Bekuzarov</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ariana-bermudez/" target="_blank">Ariana Michelle Bermudez Venegas</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                    <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html" target="_blank">Hao Li</a><sup>1,2</sup>
              </span>
                  </div>
                  <br>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>MBZUAI &nbsp;&nbsp;<sup>2</sup>Pinscreen &nbsp;&nbsp;<sup>3</sup>Adobe Research
                      <br><span style="font-weight: bold;">ICCV 2023</span></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>These authors equally contributed to the work.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2307.15958.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/max810/XMem2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2307.15958" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop>
        <!-- Your video here -->
        <source src="static/videos/teaser_6_out_of_1800_smaller.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Inspired by cases from movie industry, we present <b>XMem++</b> - an Interactive Video Object Segmentation tool that performs high-fidelity segmentation in complex and challenging scenes with <b>only a few labeled examples </b>. 
        <br><br> Someone wore a wrong shirt in the scene? A tattoo that shouldn't be there? Need to add some CGI to a very specific part of an object? 
        <br>Or maybe you want to quickly label some video segmentation datasets with unusual/unique targets?
        <br><br>This tool is for you.
        <br> We also have a dataset for benchmarking purposes: <a href="#pumavos-dataset">PUMaVOS</a>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite advancements in user-guided video segmentation, extracting complex objects consistently for highly complex scenes is still a labor-intensive task, especially for production. 
            It is not uncommon that a majority of frames need to be annotated. 
            We introduce a novel semi-supervised video object segmentation (SSVOS) model, <span style="font-weight: bold;">XMem++</span>, that improves existing memory-based models, with a permanent memory module. 
            Most existing methods focus on single frame annotations, while our approach can effectively handle multiple user-selected frames with varying appearances of the same object or region.
            Our method can extract highly consistent results while keeping the required number of frame annotations low. 
            We further introduce an iterative and attention-based frame suggestion mechanism, which computes the next best frame for annotation. 
            Our method is real-time and does not require retraining after each user input. 
            We also introduce a new dataset, <span style="font-weight: bold;">PUMaVOS</span> (Partial and Unusual Masks for Video Object Segmentation), which covers new challenging use cases not found in previous benchmarks. 
            We demonstrate SOTA performance on challenging (partial and multi-class) segmentation scenarios as well as long videos, while ensuring significantly fewer frame annotations than any existing method.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Overview</h2>
    <div class="content">
      <p>
        <div class="container">
          <p>XMem++ is a <strong>memory-based</strong> interactive segmentation model - this means it uses a set of reference frames/feature maps and their corresponding masks, either predicted or given as ground truth if available, to predict masks for new frames based on <strong>how similar they are to already processed frames</strong> with known segmentation.</p>
          <p>Just like XMem, we use the two types of memory inspired by the Atkinson-Shiffrin human memory model - working memory and long-term memory. The first one stores recent convolutional feature maps with rich details, and the other - heavily compressed features for long-term dependencies across frames that are far apart in the video.</p>
          <p>However, existing segmentation methods (<a href="https://arxiv.org/abs/2207.07115">XMem</a>, <a href="https://arxiv.org/abs/2207.06953">TBD</a>, <a href="https://arxiv.org/abs/2106.02638">AoT</a>, <a href="https://arxiv.org/abs/2210.09782">DeAOT</a>, <a href="https://arxiv.org/abs/2106.05210">STCN</a>, etc.) that are using memory mechanisms to predict the segmentation mask for the current frame, typically process frames one by one, and thus suffer from a common issue - &quot;jumps&quot; in visual quality, when the new ground truth annotation is encountered in the video</p>
        </div>
      </p>
    </div>
  </div>
  <div class="container hero-body" >
    <div class="container" style="border: none;" width="200%">
      <a href="static/images/architecture_explanations.jpg"><img src="static/images/architecture_explanations.jpg"></a>
    </div>
    <p class="has-text-centered" style="font-style: italic;">XMem++ architecture.</p>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Frame annotation candidates selector</h2>
    <div class="content">
      <p>
        <div class="container">
          <p>XMem++ is equipped with a single yet powerful algorithm to select which frames the user should annotate next to maximize performance and save time. It is based on an idea of <b>diversity</b> - to select the frames that capture the most variety of the target object's appearance - to <b>maximize the information</b> the network will get with them annotated.</p>
          <p>It takes into account <b>which object</b> we are trying to segment and recommends frames that have the most variety in that specific object's appearance.</p>
        </div>
      </p>
    </div>
  </div>
  <div class="container is-max-desktop hero-body">
    <div class="container" style="border: none;" width="100%">
      <a href="static/images/frame_selector_showcase.jpg"><img src="static/images/frame_selector_showcase.jpg"></a>
    </div>
    <p class="has-text-centered" style="font-style: italic;">Frame selector at work - notice how given a different person in rows 1) and 2) it selects different frames where <b>that specific person</b> moves. In the third row the frames capture the target object in a variety of backgrounds, expressions and lighting conditions.</p>
  </div>
</section>


<!-- Video carousel -->
<section class="section hero is-small">
  <div class=" is-centered">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Performance</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video2" autoplay controls muted loop width="100%" height="100%">
            <!-- Your video file here -->
            <source src="static/videos/billie_hair_5_out_of_847_compressed.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <b>Hair</b> (highly deformable and fluid) - only <b>5</b> out of <b>847</b> frames annotated (<b>~0.6%</b>)
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video3" autoplay controls muted loop width="100%" height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/sweater_5_out_of_552_compressed.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <b>Sweater</b> (deformation, rotation) - only <b>5</b> out of <b>552</b> frames annotated (<b>~0.9%</b>)
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video1" autoplay controls muted loop width="100%" height="100%">
            <!-- Your video file here -->
            <source src="static/videos/shirt_11_out_of_918_compressed.mp4"
            type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            <b>Shirt</b> (deformation, rotation, occlusion, similar objects) - only <b>11</b> out of <b>918</b> frames annotated (<b>~1.2%</b>)
          </h2>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop" style="padding-top: 1em">
      <ul class="spaced">
        <li><span class="subtitle">XMem++ achieves great visual results for a variety of use-cases, using only a few annotations (typically &lt; <b>10</b> for a <b>30-60s</b> video)</span></li>
        <li><span class="subtitle">It treats ground truth annotations as references and infers masks for new frames based on their similarity to known references.</span></li>
        <li><span class="subtitle">No fine-tuning, plug&amp;play, <b>30FPS</b> on <b>480p</b> video on a single GPU - check out the <a href="https://github.com/max810/XMem2">[Code]</a> on GitHub for more details.</span></li>
      </ul>
    </div>
  </div>
</section>
<!-- End video carousel -->



<!-- Youtube video -->
<section class="section hero is-small is-light">
  <div class="">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/3X3TUP4vKcc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->
<section class="section is-small hero" id="pumavos-dataset">
  <div class="">
    <div class="container is-max-desktop">
      <h2 class="title is-3">PUMaVOS dataset</h2>
      <div class="container" style="margin-bottom: 20px;">
        <p class="subtitle">We used XMem++ to collect and annotate <b>PUMaVOS</b> - a dataset of challenging and practical use cases inspired by the movie production industry.</p>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="hero-body container is-max-desktop">
          <table class="imagegrid">
          <tr>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/billie_shoes_square.gif" alt="Billie Shoes">
                  <!-- <p align="center">Shoes <br/> (<i>"billie_shoes" video</i>)</p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/chair_short_square.gif" alt="Short Chair">
                  <!-- <p align="center">Reflections <br/> <i>("chair" video)</p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/dog_tail_square.gif" alt="Dog Tail">
                  <!-- <p align="center">Body parts <br/> (<i>"dog_tail" video</i>)</p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/pants_workout_square.gif" alt="Workout Pants">
                  <!-- <p align="center">Deformable objects <br/> (<i>"pants_workout" video</i>)</p> -->
              </div>
              </td>
          </tr>
          <tr>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/skz_square.gif" alt="SKZ">
                  <!-- <p align="center">Similar objects, occlusion <br/> (<i>"skz" video</i>) </p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/tattoo_square.gif" alt="Tattoo">
                  <!-- <p align="center">Tattos/patterns <br/> (<i>"tattoo" video</i>) </p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/ice_cream_square.gif" alt="Ice Cream">
                  <!-- <p align="center">Quick motion <br/> (<i>"ice_cream" video</i>)</p> -->
              </div>
              </td>
              <td width="25%">
              <div>
                  <img class="imgcell" src="static/gifs/vlog_square.gif" alt="Vlog">
                  <!-- <p align="center">Multi-object parts <br/> (<i>"vlog" video</i>) </p> -->
              </div>
              </td>
          </tr>
          </table>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <p class="subtitle"><b>Partial and Unusual Masks for Video Object Segmentation (PUMaVOS)</b> dataset has the following properties:</p>
      <ul>
        <li><b>23</b> videos, <b>19770</b> densely-annotated frames;</li>
        <li>Covers complex practical use cases such as object parts, frequent occlusions, fast motion, deformable objects and more;</li>
        <li>Average length of the video is <b>659 frames</b> or <b>28s</b>, with the longer ones spanning <b>1min</b>;</li>
        <li>Fully densely annotated at <b>30FPS</b>;</li>
        <li>Benchmark-oriented: no separation into training/test, designed to be as diverse as possible to test your models;</li>
        <li>100% open and free to download.</li>
      </ul>
      <br>
      <h3 id="download" class="subtitle"><b>Download</b></h3>
      <p>Separate sequences and masks are available here: <a href="https://drive.google.com/drive/folders/1Q7gSCCgemUyweu-7-Yb9G_W55Muq5-bC?usp=drive_link">[Google Drive]</a></p>
      <br>
      <p>PUMaVOS <code>.zip</code> download link: <a href="https://drive.google.com/file/d/1zMaBt0Nc9aGVogRpiSviSNHbEVHPkXxW/view?usp=drive_link">[Google Drive]</a></p>
      <br>
      <h3 id="license" class="subtitle"><b>PUMaVOS license</b></h3>
      <a href="https://creativecommons.org/licenses/by/4.0/"><img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png" width="15%"></a>
      <p>PUMaVOS is released under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</a>, - you can use it for any purpose (including <b>commercial</b>), you only need to credit the authors (us) whenever you do and indicate if you&#39;ve made any modifications. See the full license text in <a href="LICENSE_PUMaVOS">LICENSE_PUMaVOS</a></p>
      <br>
      <p>PUMaVOS contains <b>5</b> videos from YouTube. We do not claim ownership of them, and here are the links to the original videos and their creators:</p>
      <ul>
        <li><a href="https://youtu.be/YDes3xOd9LQ?t=419">"pants_workout" sequence</a> by <a href="https://www.youtube.com/@tarasbody">Tara's body</a> channel</li>
        <li><a href="https://youtu.be/RUQl6YcMalg">"billie_shoes" and "billie_hair" sequences</a> by <a href="https://www.youtube.com/channel/UCiGm_E4ZwYSHV3bcW1pnSeQ">Billie Eilish</a> channel</li>
        <li><a href="https://youtu.be/8DHP5XZPSjc">"nails" sequence</a> by <a href="https://www.youtube.com/@AlexandrasGirlyTalk">AlexandrasGirlyTalk</a> channel</li>
        <li><a href="https://youtu.be/voBRTzZ0FNo">"skz" sequence</a> by <a href="https://www.youtube.com/channel/UC9rMiEjNaCSsebs31MRDCRA">Stray Kids</a> channel</li>
      </ul>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-bottom: 2em;">
    <h2 class="title">BibTeX</h2>
    <p>If you are using <b>XMem++</b> <b>PUMaVOS</b> dataset in your work, please cite us:</p>
    <pre><code>@misc{bekuzarov2023xmem,
    title={XMem++: Production-level Video Segmentation From Few Annotated Frames}, 
    author={Maksym Bekuzarov and Ariana Bermudez and Joon-Young Lee and Hao Li},
    year={2023},
    eprint={2307.15958},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre></div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
            <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
